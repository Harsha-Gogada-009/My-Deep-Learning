{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pmurLR5Gbh3F"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OKDoaTWebyMk",
    "outputId": "cc3527b8-78d4-42db-8fe2-7b2835912a48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 25\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Image preprocessing modules\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "# CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='root=\"C:\\\\Users\\\\harsh\\\\Z\\\\BootCamp',\n",
    "                                             train=True,\n",
    "                                             transform=transform,\n",
    "                                             download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='root=\"C:\\\\Users\\\\harsh\\\\Z\\\\BootCamp',\n",
    "                                            train=False,\n",
    "                                            transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fvt1OKkcMUa",
    "outputId": "23012caa-0ddf-4e05-afca-da8b8920b79f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x79d303c69bd0>\n"
     ]
    }
   ],
   "source": [
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E55nkE1XcRy4"
   },
   "outputs": [],
   "source": [
    "def conv3x3(in_channles , out_channles , stride = 1):\n",
    "  return nn.Conv2d(in_channles , out_channles , kernel_size= 3 , stride = stride , padding = 1)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "  def __init__(self, in_channles , out_channles , stride = 1 , downsampling = None):\n",
    "    super(ResidualBlock , self).__init__()\n",
    "    self.conv1 = conv3x3(in_channles ,  out_channles , stride )\n",
    "    self.bn1 = nn.BatchNorm2d(out_channles)\n",
    "    self.relu = nn.ReLU(inplace = True)\n",
    "    self.conv2 = conv3x3(out_channles ,  out_channles )\n",
    "    self.bn2 = nn.BatchNorm2d(out_channles)\n",
    "    self.downsampling = downsampling\n",
    "  def forward(self , x):\n",
    "    residual = x.clone()\n",
    "    out = self.conv1(x)\n",
    "    out = self.bn1(out)\n",
    "    out = self.relu(out)\n",
    "    out = self.conv2(out)\n",
    "    if self.downsampling:\n",
    "      residual = self.downsampling(x)\n",
    "    out += residual\n",
    "    out = self.relu(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHHoH5BB3X3t"
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "  def __init__(self,block , layers , num_classes = 10):\n",
    "    #layers list = [2,2,2] len(list) = layers , inside no of blocks\n",
    "    super(ResNet , self).__init__()\n",
    "    self.in_channles = 16\n",
    "    self.conv = conv3x3(3,16,)\n",
    "    self.bn = nn.BatchNorm2d(16)\n",
    "    self.relu = nn.ReLU(inplace = True)\n",
    "    self.layer1 = self.make_layer(block , 16 , layers[0] , stride = 1)\n",
    "    self.layer2 = self.make_layer(block , 32 , layers[1] , stride = 2)\n",
    "    self.layer3 = self.make_layer(block , 64 ,layers[2], stride = 2)\n",
    "    self.avg_pool = nn.AdaptiveAvgPool2d(1)  # 🔥 This ensures the output is always (batch, 64, 1, 1)\n",
    "    self.fc = nn.Linear(4096, num_classes)\n",
    "\n",
    "  def make_layer(self , block , out_channles ,blocks , stride = 1):\n",
    "    downsampling = None\n",
    "    if (self.in_channles != out_channles) or (stride != 1):\n",
    "      downsampling = nn.Sequential(conv3x3(self.in_channles , out_channles , stride = stride) , nn.BatchNorm2d(out_channles))\n",
    "\n",
    "    residual_blocks = []\n",
    "    residual_blocks.append(block(self.in_channles ,out_channles , stride = stride , downsampling = downsampling))\n",
    "    self.in_channles = out_channles\n",
    "    residual_blocks.append(block(self.in_channles ,out_channles ))\n",
    "    return nn.Sequential(*residual_blocks)\n",
    "  def forward(self , x):\n",
    "    out = self.conv(x)\n",
    "    out = self.bn(out)\n",
    "    out = self.relu(out)\n",
    "    out = self.layer1(out)\n",
    "    out = self.layer2(out)\n",
    "    out = self.layer3(out)\n",
    "    out = torch.flatten(out, 1)\n",
    "    out = self.fc(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DaDdl_vpB-y0"
   },
   "outputs": [],
   "source": [
    "model = ResNet(ResidualBlock , [2,2,2])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters() , learning_rate , )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xtrNiRS6CrDU",
    "outputId": "0a66b1fa-e354-44c6-daa4-60fdb83ff361"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Step [100/500] Loss: 1.5393\n",
      "Epoch [1/25], Step [200/500] Loss: 1.4437\n",
      "Epoch [1/25], Step [300/500] Loss: 1.3520\n",
      "Epoch [1/25], Step [400/500] Loss: 1.4245\n",
      "Epoch [1/25], Step [500/500] Loss: 1.2883\n",
      "Epoch [2/25], Step [100/500] Loss: 1.0088\n",
      "Epoch [2/25], Step [200/500] Loss: 1.0584\n",
      "Epoch [2/25], Step [300/500] Loss: 0.7238\n",
      "Epoch [2/25], Step [400/500] Loss: 0.9748\n",
      "Epoch [2/25], Step [500/500] Loss: 0.8777\n",
      "Epoch [3/25], Step [100/500] Loss: 1.1098\n",
      "Epoch [3/25], Step [200/500] Loss: 1.0552\n",
      "Epoch [3/25], Step [300/500] Loss: 0.7856\n",
      "Epoch [3/25], Step [400/500] Loss: 0.7611\n",
      "Epoch [3/25], Step [500/500] Loss: 0.8873\n",
      "Epoch [4/25], Step [100/500] Loss: 0.7621\n",
      "Epoch [4/25], Step [200/500] Loss: 0.9317\n",
      "Epoch [4/25], Step [300/500] Loss: 0.7054\n",
      "Epoch [4/25], Step [400/500] Loss: 0.8434\n",
      "Epoch [4/25], Step [500/500] Loss: 0.5813\n",
      "Epoch [5/25], Step [100/500] Loss: 0.6892\n",
      "Epoch [5/25], Step [200/500] Loss: 0.7049\n",
      "Epoch [5/25], Step [300/500] Loss: 0.7672\n",
      "Epoch [5/25], Step [400/500] Loss: 0.8132\n",
      "Epoch [5/25], Step [500/500] Loss: 0.7297\n",
      "Epoch [6/25], Step [100/500] Loss: 0.6364\n",
      "Epoch [6/25], Step [200/500] Loss: 0.7865\n",
      "Epoch [6/25], Step [300/500] Loss: 0.5928\n",
      "Epoch [6/25], Step [400/500] Loss: 0.4655\n",
      "Epoch [6/25], Step [500/500] Loss: 0.4403\n",
      "Epoch [7/25], Step [100/500] Loss: 0.5477\n",
      "Epoch [7/25], Step [200/500] Loss: 0.5133\n",
      "Epoch [7/25], Step [300/500] Loss: 0.7441\n",
      "Epoch [7/25], Step [400/500] Loss: 0.6011\n",
      "Epoch [7/25], Step [500/500] Loss: 0.4661\n",
      "Epoch [8/25], Step [100/500] Loss: 0.5726\n",
      "Epoch [8/25], Step [200/500] Loss: 0.4476\n",
      "Epoch [8/25], Step [300/500] Loss: 0.6199\n",
      "Epoch [8/25], Step [400/500] Loss: 0.8033\n",
      "Epoch [8/25], Step [500/500] Loss: 0.4251\n",
      "Epoch [9/25], Step [100/500] Loss: 0.4571\n",
      "Epoch [9/25], Step [200/500] Loss: 0.5436\n",
      "Epoch [9/25], Step [300/500] Loss: 0.5706\n",
      "Epoch [9/25], Step [400/500] Loss: 0.6833\n",
      "Epoch [9/25], Step [500/500] Loss: 0.5336\n",
      "Epoch [10/25], Step [100/500] Loss: 0.5651\n",
      "Epoch [10/25], Step [200/500] Loss: 0.3883\n",
      "Epoch [10/25], Step [300/500] Loss: 0.5087\n",
      "Epoch [10/25], Step [400/500] Loss: 0.4820\n",
      "Epoch [10/25], Step [500/500] Loss: 0.3511\n",
      "Epoch [11/25], Step [100/500] Loss: 0.4722\n",
      "Epoch [11/25], Step [200/500] Loss: 0.5208\n",
      "Epoch [11/25], Step [300/500] Loss: 0.4876\n",
      "Epoch [11/25], Step [400/500] Loss: 0.5823\n",
      "Epoch [11/25], Step [500/500] Loss: 0.4582\n",
      "Epoch [12/25], Step [100/500] Loss: 0.5133\n",
      "Epoch [12/25], Step [200/500] Loss: 0.4638\n",
      "Epoch [12/25], Step [300/500] Loss: 0.3376\n",
      "Epoch [12/25], Step [400/500] Loss: 0.4418\n",
      "Epoch [12/25], Step [500/500] Loss: 0.5962\n",
      "Epoch [13/25], Step [100/500] Loss: 0.4590\n",
      "Epoch [13/25], Step [200/500] Loss: 0.3982\n",
      "Epoch [13/25], Step [300/500] Loss: 0.5854\n",
      "Epoch [13/25], Step [400/500] Loss: 0.5025\n",
      "Epoch [13/25], Step [500/500] Loss: 0.4533\n",
      "Epoch [14/25], Step [100/500] Loss: 0.3594\n",
      "Epoch [14/25], Step [200/500] Loss: 0.5187\n",
      "Epoch [14/25], Step [300/500] Loss: 0.4640\n",
      "Epoch [14/25], Step [400/500] Loss: 0.5469\n",
      "Epoch [14/25], Step [500/500] Loss: 0.4300\n",
      "Epoch [15/25], Step [100/500] Loss: 0.3669\n",
      "Epoch [15/25], Step [200/500] Loss: 0.4746\n",
      "Epoch [15/25], Step [300/500] Loss: 0.3951\n",
      "Epoch [15/25], Step [400/500] Loss: 0.5236\n",
      "Epoch [15/25], Step [500/500] Loss: 0.5028\n",
      "Epoch [16/25], Step [100/500] Loss: 0.2537\n",
      "Epoch [16/25], Step [200/500] Loss: 0.4744\n",
      "Epoch [16/25], Step [300/500] Loss: 0.4858\n",
      "Epoch [16/25], Step [400/500] Loss: 0.5606\n",
      "Epoch [16/25], Step [500/500] Loss: 0.3138\n",
      "Epoch [17/25], Step [100/500] Loss: 0.2283\n",
      "Epoch [17/25], Step [200/500] Loss: 0.2616\n",
      "Epoch [17/25], Step [300/500] Loss: 0.4246\n",
      "Epoch [17/25], Step [400/500] Loss: 0.4324\n",
      "Epoch [17/25], Step [500/500] Loss: 0.3712\n",
      "Epoch [18/25], Step [100/500] Loss: 0.4228\n",
      "Epoch [18/25], Step [200/500] Loss: 0.5138\n",
      "Epoch [18/25], Step [300/500] Loss: 0.2837\n",
      "Epoch [18/25], Step [400/500] Loss: 0.3808\n",
      "Epoch [18/25], Step [500/500] Loss: 0.3656\n",
      "Epoch [19/25], Step [100/500] Loss: 0.4067\n",
      "Epoch [19/25], Step [200/500] Loss: 0.3522\n",
      "Epoch [19/25], Step [300/500] Loss: 0.3273\n",
      "Epoch [19/25], Step [400/500] Loss: 0.3583\n",
      "Epoch [19/25], Step [500/500] Loss: 0.4328\n",
      "The new learning rate is 0.0005\n",
      "Epoch [20/25], Step [100/500] Loss: 0.2843\n",
      "Epoch [20/25], Step [200/500] Loss: 0.3088\n",
      "Epoch [20/25], Step [300/500] Loss: 0.2961\n",
      "Epoch [20/25], Step [400/500] Loss: 0.4460\n",
      "Epoch [20/25], Step [500/500] Loss: 0.2740\n",
      "Epoch [21/25], Step [100/500] Loss: 0.3582\n",
      "Epoch [21/25], Step [200/500] Loss: 0.2708\n",
      "Epoch [21/25], Step [300/500] Loss: 0.2739\n",
      "Epoch [21/25], Step [400/500] Loss: 0.3382\n",
      "Epoch [21/25], Step [500/500] Loss: 0.3285\n",
      "Epoch [22/25], Step [100/500] Loss: 0.4504\n",
      "Epoch [22/25], Step [200/500] Loss: 0.3782\n",
      "Epoch [22/25], Step [300/500] Loss: 0.2824\n",
      "Epoch [22/25], Step [400/500] Loss: 0.3372\n",
      "Epoch [22/25], Step [500/500] Loss: 0.3671\n",
      "Epoch [23/25], Step [100/500] Loss: 0.4078\n",
      "Epoch [23/25], Step [200/500] Loss: 0.1768\n",
      "Epoch [23/25], Step [300/500] Loss: 0.2575\n",
      "Epoch [23/25], Step [400/500] Loss: 0.2426\n",
      "Epoch [23/25], Step [500/500] Loss: 0.1291\n",
      "Epoch [24/25], Step [100/500] Loss: 0.3200\n",
      "Epoch [24/25], Step [200/500] Loss: 0.2566\n",
      "Epoch [24/25], Step [300/500] Loss: 0.1615\n",
      "Epoch [24/25], Step [400/500] Loss: 0.2749\n",
      "Epoch [24/25], Step [500/500] Loss: 0.2939\n",
      "Epoch [25/25], Step [100/500] Loss: 0.2498\n",
      "Epoch [25/25], Step [200/500] Loss: 0.2556\n",
      "Epoch [25/25], Step [300/500] Loss: 0.2924\n",
      "Epoch [25/25], Step [400/500] Loss: 0.2609\n",
      "Epoch [25/25], Step [500/500] Loss: 0.2846\n"
     ]
    }
   ],
   "source": [
    "decay = 0\n",
    "model.train()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  #decay the learning rate by facctor of 0.5 every 20 epochs\n",
    "  if ((epoch + 1) % 20 == 0):\n",
    "    decay +=1\n",
    "    optimizer.param_groups[0]['lr'] = (learning_rate * 0.5**decay)\n",
    "    print(\"The new learning rate is {}\".format(optimizer.param_groups[0][\"lr\"]))\n",
    "\n",
    "  for i , (images , labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "faDVH2IUFIXC",
    "outputId": "74f9e24d-5611-436b-933f-f1b49c761082"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 86.7 %\n"
     ]
    }
   ],
   "source": [
    "#Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IuaKsBlE2SU"
   },
   "source": [
    "Summary of the Discussion on Residual Networks (ResNets)\n",
    "Skip Connections in ResNets\n",
    "The final output of each residual block is:\n",
    "H(x)=F(x)+x\n",
    "This helps in avoiding vanishing gradients and makes training deep networks easier.\n",
    "Why Do We Add\n",
    "𝑥\n",
    "x (Skip Connection)?\n",
    "\n",
    "It ensures better gradient flow, preventing the network from learning very small updates.\n",
    "It helps retain low-level features while still learning new ones.\n",
    "It allows the network to easily skip layers if they are not needed, making optimization easier.\n",
    "Why Do We Add\n",
    "𝑥\n",
    "x Multiple Times?\n",
    "\n",
    "Since ResNet is made of multiple residual blocks, each block has its own skip connection.\n",
    "Each residual block builds on the previous one by adding its own transformation to the accumulated result.\n",
    "This ensures better gradient flow, easier optimization, and improved feature preservation at every stage of the network.\n",
    "Downsampling in Residual Blocks\n",
    "\n",
    "If the input size changes (e.g., due to a stride > 1), downsampling is used to match the dimensions before adding the residual connection.\n",
    "This is typically done using a 1x1 convolution.\n",
    "Key Takeaways\n",
    "✅ ResNets prevent vanishing gradients and improve deep network performance.\n",
    "✅ Skip connections help with optimization and feature retention.\n",
    "✅ Adding\n",
    "𝑥\n",
    "x multiple times allows gradients to flow through deep networks effectively.\n",
    "\n",
    "Would you like a code improvement suggestion for your implementation? 🚀"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
